{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import shutil\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"adjective\"]=[\n",
    "\"absent\",\n",
    "\"beautiful\",\n",
    "\"all\",\n",
    "\"big\",\n",
    "\"any\",\n",
    "\"another\",\n",
    "\"boring\",\n",
    "\"deaf\",\n",
    "\"dark\",\n",
    "\"difficult\",\n",
    "\"easy\",\n",
    "\"full\",\n",
    "\"fat\",\n",
    "\"fast\",\n",
    "\"hard\",\n",
    "\"hot\",\n",
    "\"less\",\n",
    "\"low\",\n",
    "\"loud\",\n",
    "\"more\",\n",
    "\"next\",\n",
    "\"old\",\n",
    "\"other\",\n",
    "\"permanent\",\n",
    "\"safe\",\n",
    "\"same\",\n",
    "\"slow\",\n",
    "\"tall\",\n",
    "\"short\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"adverb\"]=[\n",
    "\"again\",\n",
    "\"always\",\n",
    "\"often\",\n",
    "\"sometimes\",\n",
    "\"through\",\n",
    "\"up\",\n",
    "\"together\",\n",
    "\"where\",\n",
    "\"why\",\n",
    "\"when\",\n",
    "\"how\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"preposition\"]=[\n",
    "\"about\",\n",
    "\"among\",\n",
    "\"at\",\n",
    "\"around\",\n",
    "\"before\",\n",
    "\"below\",\n",
    "\"down\",\n",
    "\"from \",\n",
    "\"near\",\n",
    "\"in\",\n",
    "\"on\",\n",
    "\"out\",\n",
    "\"to\",\n",
    "\"under\",\n",
    "\"with\",\n",
    "\"without\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"pronoun\"]=[\n",
    "\"he\",\n",
    "\"her\",\n",
    "\"hers\",\n",
    "\"him\",\n",
    "\"his\",\n",
    "\"I\",\n",
    "\"it\",\n",
    "\"me\",\n",
    "\"our\",\n",
    "\"those\",\n",
    "\"we\",\n",
    "\"whose\",\n",
    "\"who\",\n",
    "\"you\",\n",
    "\"your\",\n",
    "\"us\",\n",
    "\"what\",\n",
    "\"them\",\n",
    "\"my\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"verb\"]=[\n",
    "\"worry\",\n",
    "\"wear\",\n",
    "\"wait\",\n",
    "\"try\",\n",
    "\"touch\",\n",
    "\"take\",\n",
    "\"surprise\",\n",
    "\"stop\",\n",
    "\"start\",\n",
    "\"spill\",\n",
    "\"smile\",\n",
    "\"shake\",\n",
    "\"show\",\n",
    "\"ride\",\n",
    "\"remember\",\n",
    "\"put\",\n",
    "\"push\",\n",
    "\"pour\",\n",
    "\"press\",\n",
    "\"play\",\n",
    "\"pay\",\n",
    "\"move\",\n",
    "\"miss\",\n",
    "\"love\",\n",
    "\"laugh\",\n",
    "\"kick\",\n",
    "\"jump\",\n",
    "\"keep\",\n",
    "\"increase\",\n",
    "\"hide\",\n",
    "\"hate\",\n",
    "\"go\",\n",
    "\"give\",\n",
    "\"forget\",\n",
    "\"feel\",\n",
    "\"enjoy\",\n",
    "\"like\",\n",
    "\"decrease\",\n",
    "\"cry\",\n",
    "\"hook\",\n",
    "\"continue\",\n",
    "\"close\",\n",
    "\"open\",\n",
    "\"call\",\n",
    "\"carry\",\n",
    "\"catch\",\n",
    "\"bend\",\n",
    "\"break\",\n",
    "\"bath\",\n",
    "\"run\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"noun\"]=[\n",
    "\"brother\",\n",
    "\"family\",\n",
    "\"freind\",\n",
    "\"girkl\",\n",
    "\"husvband\",\n",
    "\"mother\",\n",
    "\"mother in law\",\n",
    "\"name\",\n",
    "\"sister\",\n",
    "\"wife\",\n",
    "\"women\",\n",
    "\"man\",\n",
    "\"daiughter\",\n",
    "\"son\",\n",
    "\"children\",\n",
    "\"cap\",\n",
    "\"helment\",\n",
    "\"pants\",\n",
    "\"umbrella\",\n",
    "\"t shirt\",\n",
    "\"sweater\",\n",
    "\"skirt\",\n",
    "\"mask\",\n",
    "\"shoes\",\n",
    "\"dress\",\n",
    "\"evening \",\n",
    "\"morning\",\n",
    "\"afternoon\",\n",
    "\"today\",\n",
    "\"yesterday\",\n",
    "\"tomorrow\",\n",
    "\"night\",\n",
    "\"late\",\n",
    "\"time\",\n",
    "\"bag\",\n",
    "\"key\",\n",
    "\"phone\",\n",
    "\"ticket \",\n",
    "\"money\",\n",
    "\"trash can\",\n",
    "\"carrot\",\n",
    "\"onion\",\n",
    "\"potato\",\n",
    "\"tomato\",\n",
    "\"apple\",\n",
    "\"banana\",\n",
    "\"grape\",\n",
    "\"guava\",\n",
    "\"red\",\n",
    "\"green\",\n",
    "\"blue\",\n",
    "\"yellow\",\n",
    "\"orange\",\n",
    "\"pink\",\n",
    "\"brown\",\n",
    "\"white\",\n",
    "\"black\",\n",
    "\"die\",\n",
    "\"disease\",\n",
    "\"ill\",\n",
    "\"sick\",\n",
    "\"wound\",\n",
    "\"alive\",\n",
    "\"cold\",\n",
    "\"cough\",\n",
    "\"gate\",\n",
    "\"roof\",\n",
    "\"bed\",\n",
    "\"door\",\n",
    "\"ladder\",\n",
    "\"window\",\n",
    "\"toilet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "#                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "#                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "#                              ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection (Run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'slr_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-40a898ae4037>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slr_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"slr_data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrammar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'slr_data'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"slr_data\")\n",
    "data_path=\"slr_data\"\n",
    "for grammar in words:\n",
    "    os.mkdir(data_path+\"/\"+grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data per word\n",
    "no_sequences = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest Time between each action recording\n",
    "pause_time = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcrop = 0\n",
    "wcrop = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# action is the word variable, action=\"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i'm using word \"when\" here\n",
    "action=\"family\"\n",
    "#grammar is the grammar category the word is in:\n",
    "#choose from ['adjective', 'adverb', 'preposition', 'pronoun', 'verb', 'noun']\n",
    "grammar=\"\"\n",
    "for i in words:\n",
    "    if action in words[i]:\n",
    "        grammar=i\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin capture for word (if something fails, just rerun cell, data is saved automatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Press Q to quit while capturing. Word won't be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=False\n",
    "nopose=torch.zeros(3,sequence_length,19)\n",
    "noleft=torch.zeros(3,sequence_length,21)\n",
    "noright=torch.zeros(3,sequence_length,21)\n",
    "\n",
    "poselist=torch.zeros(3,sequence_length,19)\n",
    "leftlist=torch.zeros(3,sequence_length,21)\n",
    "rightlist=torch.zeros(3,sequence_length,21)\n",
    "biglist=torch.zeros(no_sequences,3,sequence_length,61)\n",
    "cap = cv2.VideoCapture(0)\n",
    "bigresults=[]\n",
    "tempresults=[]\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    sequence=0\n",
    "    while(sequence<no_sequences):\n",
    "        for frame_pause in range(pause_time):\n",
    "            ret, frame = cap.read()\n",
    "#             frame = frame[hcrop:-hcrop,wcrop:-wcrop]\n",
    "#             print(frame.shape)\n",
    "#             frame = cv2.resize(frame, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'STARTING COLLECTION', (250,300), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((pause_time-frame_pause)//10+1), (350,350), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('n'):\n",
    "                break\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 stop=True\n",
    "#                 break\n",
    "        #record video of 32 frames\n",
    "        tempresults = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "#             frame = frame[hcrop:-hcrop,wcrop:-wcrop]\n",
    "#             print(frame.shape)\n",
    "#             frame = cv2.resize(frame, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            tempresults.append(results)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'RECORDING', (640,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((sequence_length-frame_num)//10+1), (870,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                stop=True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "        bigresults.append(tempresults)\n",
    "        sequence+=1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    for sequence,results in enumerate(bigresults):\n",
    "        for frame,result in enumerate(results):\n",
    "            posepoints=result.pose_landmarks.landmark if result.pose_landmarks else nopose\n",
    "            leftpoints=result.left_hand_landmarks.landmark if result.left_hand_landmarks else noleft\n",
    "            rightpoints=result.right_hand_landmarks.landmark if result.right_hand_landmarks else noright\n",
    "            if result.pose_landmarks:\n",
    "                for i in range(len(posepoints)):\n",
    "                    if(i>18):\n",
    "                        break\n",
    "                    poselist[0][frame][i]=posepoints[i].x\n",
    "                    poselist[1][frame][i]=posepoints[i].y\n",
    "                    poselist[2][frame][i]=posepoints[i].z\n",
    "                poselist[0][frame][17]=posepoints[23].x; poselist[0][frame][18]=posepoints[24].x;\n",
    "                poselist[1][frame][17]=posepoints[23].y; poselist[1][frame][18]=posepoints[24].y;\n",
    "                poselist[2][frame][17]=posepoints[23].z; poselist[2][frame][18]=posepoints[24].z;\n",
    "            else:\n",
    "                poselist[0][frame]=torch.zeros(18)\n",
    "                poselist[1][frame]=torch.zeros(18)\n",
    "                poselist[2][frame]=torch.zeros(18)\n",
    "            if result.left_hand_landmarks:\n",
    "                for i in range(len(leftpoints)):\n",
    "                    leftlist[0][frame][i]=leftpoints[i].x\n",
    "                    leftlist[1][frame][i]=leftpoints[i].y\n",
    "                    leftlist[2][frame][i]=leftpoints[i].z\n",
    "            else:\n",
    "                leftlist[0][frame]=torch.zeros(21)\n",
    "                leftlist[1][frame]=torch.zeros(21)\n",
    "                leftlist[2][frame]=torch.zeros(21)\n",
    "            if result.right_hand_landmarks:\n",
    "                for i in range(len(rightpoints)):\n",
    "                    rightlist[0][frame][i]=rightpoints[i].x\n",
    "                    rightlist[1][frame][i]=rightpoints[i].y\n",
    "                    rightlist[2][frame][i]=rightpoints[i].z\n",
    "            else:\n",
    "                rightlist[0][frame]=torch.zeros(21)\n",
    "                rightlist[1][frame]=torch.zeros(21)\n",
    "                rightlist[2][frame]=torch.zeros(21)\n",
    "                \n",
    "        biglist[sequence]=torch.cat((leftlist,poselist,rightlist),2)\n",
    "\n",
    "torch.save(biglist,\"test_data/\"+action+\".pt\")\n",
    "torch.save(biglist,\"slr_data/\"+grammar+\"/\"+action+\".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancerslr",
   "language": "python",
   "name": "cancerslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
