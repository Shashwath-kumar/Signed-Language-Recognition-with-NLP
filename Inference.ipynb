{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "from torch_geometric_temporal.dataset import MTMDatasetLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric_temporal.nn.attention import AAGCN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "#                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "#                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "#                              ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data per word\n",
    "no_sequences = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest Time between each action recording\n",
    "pause_time = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcrop = 0\n",
    "wcrop = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for subdir, dirs, files in os.walk('./test_data'):\n",
    "    a=files\n",
    "#class labels list\n",
    "labels=[x[:-3] for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 61\n",
    "wordsz=len(a)\n",
    "frames=24\n",
    "classes=wordsz\n",
    "topn=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_push(tensor, x):\n",
    "    return torch.cat((tensor[1:], x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Electra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n",
    "electra_model = ElectraForMaskedLM.from_pretrained('google/electra-small-generator').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing AAGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load edge index tensor\n",
    "edge_index=torch.load(\"cool_shit/new_edge_index.pt\")\n",
    "edge_index=edge_index.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(CoolGCN, self).__init__()\n",
    "        self.r1 = AAGCN(3,32, torch.LongTensor(edge_index), node_features)\n",
    "        self.r2 = AAGCN(32, 32,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r3 = AAGCN(32, 64,  torch.LongTensor(edge_index), node_features,stride =2)\n",
    "        self.r4 = AAGCN(64, 64,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r5 = AAGCN(64, 128,  torch.LongTensor(edge_index), node_features,stride = 2)\n",
    "        self.r6 = AAGCN(128, 128,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r7 = AAGCN(128, 256,  torch.LongTensor(edge_index), node_features,stride = 2)\n",
    "        self.r8 = AAGCN(256, 256,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r9 = AAGCN(256, 768,  torch.LongTensor(edge_index), node_features,stride = 3)\n",
    "        self.r10 = AAGCN(768, 768,  torch.LongTensor(edge_index), node_features)\n",
    "        self.linear = torch.nn.Linear(nodes, classes)\n",
    "        self.dropout = torch.nn.Dropout(0.4)\n",
    "    def forward(self, x):\n",
    "        h = self.r1(x)\n",
    "        h = self.r2(h)\n",
    "        h = self.r3(h)\n",
    "        h = self.r4(h)\n",
    "        h = self.r5(h)\n",
    "        h = self.r6(h)\n",
    "        h = self.r7(h)\n",
    "        h = self.r8(h)\n",
    "        h = self.r9(h)\n",
    "        h = self.r10(h)\n",
    "        h = h.mean(1)\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#load model to device\n",
    "model = CoolGCN(node_features = nodes).to(device)\n",
    "#load model from file\n",
    "model.load_state_dict(torch.load(\"./models/fulledgemodel.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrakeys = dict()\n",
    "for w in a:\n",
    "    if w == 't-shirt.pt':\n",
    "        word = 'shirt.pt'\n",
    "    else:\n",
    "        word = w\n",
    "    for i in range(9999999):\n",
    "        if ''.join(electra_tokenizer.decode(i).split()) == word[:-3].lower():\n",
    "            electrakeys[w[:-3]] = i\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return '\\n'.join(tokens[:top_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoints(result,posepoints,leftpoints,rightpoints,poselist,rightlist,leftlist,flag):\n",
    "    if result.pose_landmarks:\n",
    "        temptensor = torch.zeros(3,1,19)\n",
    "        for i in range(len(posepoints)):\n",
    "            if(i>18):\n",
    "                break\n",
    "            temptensor[0][0][i]=posepoints[i].x\n",
    "            temptensor[1][0][i]=posepoints[i].y\n",
    "            temptensor[2][0][i]=posepoints[i].z\n",
    "        temptensor[0][0][17]=posepoints[23].x\n",
    "        temptensor[0][0][18]=posepoints[24].x\n",
    "        temptensor[1][0][17]=posepoints[23].y\n",
    "        temptensor[1][0][18]=posepoints[24].y\n",
    "        temptensor[2][0][17]=posepoints[23].z\n",
    "        temptensor[2][0][18]=posepoints[24].z\n",
    "        poselist[0]=tensor_push(poselist[0], temptensor[0])\n",
    "        poselist[1]=tensor_push(poselist[1], temptensor[1])\n",
    "        poselist[2]=tensor_push(poselist[2], temptensor[2])\n",
    "    else:\n",
    "        poselist[0]=tensor_push(poselist[0], torch.zeros(1,19))\n",
    "        poselist[1]=tensor_push(poselist[1], torch.zeros(1,19))\n",
    "        poselist[2]=tensor_push(poselist[2], torch.zeros(1,19))\n",
    "\n",
    "    if result.left_hand_landmarks:\n",
    "        temptensor = torch.zeros(3,1,21)\n",
    "        for i in range(len(leftpoints)):\n",
    "            temptensor[0][0][i]=leftpoints[i].x\n",
    "            temptensor[1][0][i]=leftpoints[i].y\n",
    "            temptensor[2][0][i]=leftpoints[i].z\n",
    "        leftlist[0]=tensor_push(leftlist[0], temptensor[0])\n",
    "        leftlist[1]=tensor_push(leftlist[1], temptensor[1])\n",
    "        leftlist[2]=tensor_push(leftlist[2], temptensor[2])\n",
    "    else:\n",
    "        leftlist[0]=tensor_push(leftlist[0], torch.zeros(1,21))\n",
    "        leftlist[1]=tensor_push(leftlist[1], torch.zeros(1,21))\n",
    "        leftlist[2]=tensor_push(leftlist[2], torch.zeros(1,21))\n",
    "        flag+=1\n",
    "    if result.right_hand_landmarks:\n",
    "        temptensor = torch.zeros(3,1,21)\n",
    "        for i in range(len(rightpoints)):\n",
    "            temptensor[0][0][i]=rightpoints[i].x\n",
    "            temptensor[1][0][i]=rightpoints[i].y\n",
    "            temptensor[2][0][i]=rightpoints[i].z\n",
    "        rightlist[0]=tensor_push(rightlist[0], temptensor[0])\n",
    "        rightlist[1]=tensor_push(rightlist[1], temptensor[1])\n",
    "        rightlist[2]=tensor_push(rightlist[2], temptensor[2])\n",
    "    else:\n",
    "        rightlist[0]=tensor_push(rightlist[0], torch.zeros(1,21))\n",
    "        rightlist[1]=tensor_push(rightlist[1], torch.zeros(1,21))\n",
    "        rightlist[2]=tensor_push(rightlist[2], torch.zeros(1,21))\n",
    "        flag+=1\n",
    "        \n",
    "    return (poselist,rightlist,leftlist,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getprediction(electra_tokenizer, sentence):\n",
    "    input_ids, mask_idx = encode(electra_tokenizer, sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = electra_model(input_ids)[0]\n",
    "    return predict,mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGCNpred(biglist):\n",
    "    y_hat=model(biglist)\n",
    "    y_hat = softmax(y_hat[0][0])\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prephase = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin capture for inference. Press Q to quit while capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "csentence = 'Well, I don\\'t esteem you the'\n",
    "nsentence = csentence\n",
    "DONT_USE_NLP = max(prephase - len(csentence.split()),0)\n",
    "stop=False\n",
    "nopose=torch.zeros(3,sequence_length,19)\n",
    "noleft=torch.zeros(3,sequence_length,21)\n",
    "noright=torch.zeros(3,sequence_length,21)\n",
    "flag = 0\n",
    "wait =0\n",
    "pause_time = 50\n",
    "poselist=torch.zeros(3,sequence_length,19)\n",
    "leftlist=torch.zeros(3,sequence_length,21)\n",
    "rightlist=torch.zeros(3,sequence_length,21)\n",
    "biglist=torch.zeros(1,3,sequence_length,61)\n",
    "cap = cv2.VideoCapture(0)\n",
    "bigresults=[]\n",
    "tempresults=[]\n",
    "framecount=0\n",
    "top5 = 0\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while(True):\n",
    "        flag = 1\n",
    "        for frame_pause in range(pause_time):\n",
    "            ret, frame = cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'Pause time', (250,300), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((pause_time-frame_pause)//10+1), (350,350), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            \n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            if top5 and flag and DONT_USE_NLP>0:  \n",
    "                cv2.putText(image, '1. '+a[int(top5.indices[0])][:-3], (150,150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.putText(image, '2. '+a[int(top5.indices[1])][:-3], (150,30+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.putText(image, '3. '+a[int(top5.indices[2])][:-3], (150,2*30+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.putText(image, '4. '+a[int(top5.indices[3])][:-3], (150,3*30+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.putText(image, '5. '+a[int(top5.indices[4])][:-3], (150,4*30+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.putText(image, '6. REDO', (150,5*30+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                chosen = int(chr(cv2.waitKey(0)))\n",
    "                if chosen  != 6:                    \n",
    "                    csentence += ' '+ a[int(top5.indices[chosen-1])][:-3]\n",
    "                else:\n",
    "                    DONT_USE_NLP+=1\n",
    "                    \n",
    "                nsentence+= ' ' + a[int(top5.indices[0])][:-3]\n",
    "                flag=0\n",
    "                          \n",
    "            \n",
    "            cv2.putText(image, 'Sentence: '+csentence, (30,420), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Sentence: '+nsentence, (30,450), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 3, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('n'):\n",
    "                break\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 stop=True\n",
    "#                 break\n",
    "        #record video of 32 frames\n",
    "#         pause_time = 25\n",
    "        tempresults = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "#             frame = frame[hcrop:-hcrop,wcrop:-wcrop]\n",
    "#             print(frame.shape)\n",
    "#             frame = cv2.resize(frame, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            tempresults.append(results)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'RECORDING', (640,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((sequence_length-frame_num)//10+1), (870,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.putText(image, 'Sentence: '+csentence, (30,420), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Sentence: '+nsentence, (30,450), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 3, cv2.LINE_AA)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                stop=True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "#         bigresults.append(tempresults)\n",
    "#         sequence+=1\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "        bigresults=tempresults\n",
    "        for frame,result in enumerate(bigresults):\n",
    "            posepoints=result.pose_landmarks.landmark if result.pose_landmarks else nopose\n",
    "            leftpoints=result.left_hand_landmarks.landmark if result.left_hand_landmarks else noleft\n",
    "            rightpoints=result.right_hand_landmarks.landmark if result.right_hand_landmarks else noright\n",
    "            if result.pose_landmarks:\n",
    "                for i in range(len(posepoints)):\n",
    "                    if(i>18):\n",
    "                        break\n",
    "                    poselist[0][frame][i]=posepoints[i].x\n",
    "                    poselist[1][frame][i]=posepoints[i].y\n",
    "                    poselist[2][frame][i]=posepoints[i].z\n",
    "                poselist[0][frame][17]=posepoints[23].x;poselist[0][frame][18]=posepoints[24].x;\n",
    "                poselist[1][frame][17]=posepoints[23].y;poselist[1][frame][18]=posepoints[24].y;\n",
    "                poselist[2][frame][17]=posepoints[23].z;poselist[2][frame][18]=posepoints[24].z;\n",
    "            else:\n",
    "                poselist[0][frame]=torch.zeros(19)\n",
    "                poselist[1][frame]=torch.zeros(19)\n",
    "                poselist[2][frame]=torch.zeros(19)\n",
    "            if result.left_hand_landmarks:\n",
    "                for i in range(len(leftpoints)):\n",
    "                    leftlist[0][frame][i]=leftpoints[i].x\n",
    "                    leftlist[1][frame][i]=leftpoints[i].y\n",
    "                    leftlist[2][frame][i]=leftpoints[i].z\n",
    "            else:\n",
    "                leftlist[0][frame]=torch.zeros(21)\n",
    "                leftlist[1][frame]=torch.zeros(21)\n",
    "                leftlist[2][frame]=torch.zeros(21)\n",
    "            if result.right_hand_landmarks:\n",
    "                for i in range(len(rightpoints)):\n",
    "                    rightlist[0][frame][i]=rightpoints[i].x\n",
    "                    rightlist[1][frame][i]=rightpoints[i].y\n",
    "                    rightlist[2][frame][i]=rightpoints[i].z\n",
    "            else:\n",
    "                rightlist[0][frame]=torch.zeros(21)\n",
    "                rightlist[1][frame]=torch.zeros(21)\n",
    "                rightlist[2][frame]=torch.zeros(21)\n",
    "\n",
    "        biglist[0]=torch.cat((leftlist,poselist,rightlist),2)\n",
    "            \n",
    "            ############################################################## PREDICTION\n",
    "        biglist = biglist.cuda()\n",
    "        y_hat= getGCNpred(biglist)\n",
    "        top5=torch.topk(y_hat,5)\n",
    "\n",
    "#         if DONT_USE_NLP:\n",
    "        DONT_USE_NLP-=1\n",
    "\n",
    "        if DONT_USE_NLP<=0:\n",
    "            predict,mask_idx = getprediction(electra_tokenizer, csentence+' <mask>')\n",
    "            top_word_idx = top5.indices.tolist()\n",
    "            top_word_values = []\n",
    "            top_words = []\n",
    "            for widx in top_word_idx:\n",
    "                w = a[widx][:-3]\n",
    "                top_words.append(w)\n",
    "                top_word_values.append(predict[0, mask_idx, :][electrakeys[w]])\n",
    "\n",
    "            predwordidx = int(torch.argmax(torch.tensor(top_word_values),dim=0))\n",
    "            word2add = top_words[predwordidx]\n",
    "            csentence += ' '+ word2add\n",
    "        \n",
    "        nsentence+= ' ' + a[int(top5.indices[0])][:-3]\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, I don't esteem you the less\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, I don't esteem you the enjoy\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancerslr",
   "language": "python",
   "name": "cancerslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
