{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "from torch_geometric_temporal.dataset import MTMDatasetLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric_temporal.nn.attention import AAGCN\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "#                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "#                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "#                              ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data per word\n",
    "no_sequences = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest Time between each action recording\n",
    "pause_time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcrop = 0\n",
    "wcrop = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for subdir, dirs, files in os.walk('./test_data'):\n",
    "    a=files\n",
    "#class labels list\n",
    "labels=[x[:-3] for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 61\n",
    "wordsz=len(a)\n",
    "frames=24\n",
    "classes=wordsz\n",
    "topn=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_push(tensor, x):\n",
    "    return torch.cat((tensor[1:], x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Electra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n",
    "electra_model = ElectraForMaskedLM.from_pretrained('google/electra-small-generator').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing AAGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load edge index tensor\n",
    "edge_index=torch.load(\"cool_shit/new_edge_index.pt\")\n",
    "edge_index=edge_index.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(CoolGCN, self).__init__()\n",
    "        self.r1 = AAGCN(3,32, torch.LongTensor(edge_index), node_features)\n",
    "        self.r2 = AAGCN(32, 32,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r3 = AAGCN(32, 64,  torch.LongTensor(edge_index), node_features,stride =2)\n",
    "        self.r4 = AAGCN(64, 64,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r5 = AAGCN(64, 128,  torch.LongTensor(edge_index), node_features,stride = 2)\n",
    "        self.r6 = AAGCN(128, 128,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r7 = AAGCN(128, 256,  torch.LongTensor(edge_index), node_features,stride = 2)\n",
    "        self.r8 = AAGCN(256, 256,  torch.LongTensor(edge_index), node_features)\n",
    "        self.r9 = AAGCN(256, 768,  torch.LongTensor(edge_index), node_features,stride = 3)\n",
    "        self.r10 = AAGCN(768, 768,  torch.LongTensor(edge_index), node_features)\n",
    "        self.linear = torch.nn.Linear(nodes, classes)\n",
    "        self.dropout = torch.nn.Dropout(0.4)\n",
    "    def forward(self, x):\n",
    "        h = self.r1(x)\n",
    "        h = self.r2(h)\n",
    "        h = self.r3(h)\n",
    "        h = self.r4(h)\n",
    "        h = self.r5(h)\n",
    "        h = self.r6(h)\n",
    "        h = self.r7(h)\n",
    "        h = self.r8(h)\n",
    "        h = self.r9(h)\n",
    "        h = self.r10(h)\n",
    "        h = h.mean(1)\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#load model to device\n",
    "model = CoolGCN(node_features = nodes).to(device)\n",
    "#load model from file\n",
    "model.load_state_dict(torch.load(\"./models/fulledgemodel.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrakeys = dict()\n",
    "for w in a:\n",
    "    if w == 't-shirt.pt':\n",
    "        word = 'shirt.pt'\n",
    "    else:\n",
    "        word = w\n",
    "    for i in range(9999999):\n",
    "        if ''.join(electra_tokenizer.decode(i).split()) == word[:-3].lower():\n",
    "            electrakeys[w[:-3]] = i\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return '\\n'.join(tokens[:top_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoints(result,posepoints,leftpoints,rightpoints,poselist,rightlist,leftlist,flag):\n",
    "    if result.pose_landmarks:\n",
    "        temptensor = torch.zeros(3,1,19)\n",
    "        for i in range(len(posepoints)):\n",
    "            if(i>18):\n",
    "                break\n",
    "            temptensor[0][0][i]=posepoints[i].x\n",
    "            temptensor[1][0][i]=posepoints[i].y\n",
    "            temptensor[2][0][i]=posepoints[i].z\n",
    "        temptensor[0][0][17]=posepoints[23].x\n",
    "        temptensor[0][0][18]=posepoints[24].x\n",
    "        temptensor[1][0][17]=posepoints[23].y\n",
    "        temptensor[1][0][18]=posepoints[24].y\n",
    "        temptensor[2][0][17]=posepoints[23].z\n",
    "        temptensor[2][0][18]=posepoints[24].z\n",
    "        poselist[0]=tensor_push(poselist[0], temptensor[0])\n",
    "        poselist[1]=tensor_push(poselist[1], temptensor[1])\n",
    "        poselist[2]=tensor_push(poselist[2], temptensor[2])\n",
    "    else:\n",
    "        poselist[0]=tensor_push(poselist[0], torch.zeros(1,19))\n",
    "        poselist[1]=tensor_push(poselist[1], torch.zeros(1,19))\n",
    "        poselist[2]=tensor_push(poselist[2], torch.zeros(1,19))\n",
    "\n",
    "    if result.left_hand_landmarks:\n",
    "        temptensor = torch.zeros(3,1,21)\n",
    "        for i in range(len(leftpoints)):\n",
    "            temptensor[0][0][i]=leftpoints[i].x\n",
    "            temptensor[1][0][i]=leftpoints[i].y\n",
    "            temptensor[2][0][i]=leftpoints[i].z\n",
    "        leftlist[0]=tensor_push(leftlist[0], temptensor[0])\n",
    "        leftlist[1]=tensor_push(leftlist[1], temptensor[1])\n",
    "        leftlist[2]=tensor_push(leftlist[2], temptensor[2])\n",
    "    else:\n",
    "        leftlist[0]=tensor_push(leftlist[0], torch.zeros(1,21))\n",
    "        leftlist[1]=tensor_push(leftlist[1], torch.zeros(1,21))\n",
    "        leftlist[2]=tensor_push(leftlist[2], torch.zeros(1,21))\n",
    "        flag+=1\n",
    "    if result.right_hand_landmarks:\n",
    "        temptensor = torch.zeros(3,1,21)\n",
    "        for i in range(len(rightpoints)):\n",
    "            temptensor[0][0][i]=rightpoints[i].x\n",
    "            temptensor[1][0][i]=rightpoints[i].y\n",
    "            temptensor[2][0][i]=rightpoints[i].z\n",
    "        rightlist[0]=tensor_push(rightlist[0], temptensor[0])\n",
    "        rightlist[1]=tensor_push(rightlist[1], temptensor[1])\n",
    "        rightlist[2]=tensor_push(rightlist[2], temptensor[2])\n",
    "    else:\n",
    "        rightlist[0]=tensor_push(rightlist[0], torch.zeros(1,21))\n",
    "        rightlist[1]=tensor_push(rightlist[1], torch.zeros(1,21))\n",
    "        rightlist[2]=tensor_push(rightlist[2], torch.zeros(1,21))\n",
    "        flag+=1\n",
    "        \n",
    "    return (poselist,rightlist,leftlist,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getprediction(electra_tokenizer, sentence):\n",
    "    input_ids, mask_idx = encode(electra_tokenizer, sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = electra_model(input_ids)[0]\n",
    "    return predict,mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGCNpred(biglist):\n",
    "    y_hat=model(biglist)\n",
    "    y_hat = softmax(y_hat[0][0])\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DONT_USE_NLP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause_time = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'them'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=False\n",
    "nopose=torch.zeros(3,sequence_length,19)\n",
    "noleft=torch.zeros(3,sequence_length,21)\n",
    "noright=torch.zeros(3,sequence_length,21)\n",
    "\n",
    "poselist=torch.zeros(3,sequence_length,19)\n",
    "leftlist=torch.zeros(3,sequence_length,21)\n",
    "rightlist=torch.zeros(3,sequence_length,21)\n",
    "biglist=torch.zeros(no_sequences,3,sequence_length,61)\n",
    "cap = cv2.VideoCapture(0)\n",
    "bigresults=[]\n",
    "tempresults=[]\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    sequence=0\n",
    "    while(sequence<no_sequences):\n",
    "        for frame_pause in range(pause_time):\n",
    "            ret, frame = cap.read()\n",
    "#             frame = frame[hcrop:-hcrop,wcrop:-wcrop]\n",
    "#             print(frame.shape)\n",
    "#             frame = cv2.resize(frame, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'STARTING COLLECTION', (250,300), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((pause_time-frame_pause)//10+1), (350,350), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('n'):\n",
    "                break\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 stop=True\n",
    "#                 break\n",
    "        #record video of 32 frames\n",
    "        tempresults = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "#             frame = frame[hcrop:-hcrop,wcrop:-wcrop]\n",
    "#             print(frame.shape)\n",
    "#             frame = cv2.resize(frame, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            tempresults.append(results)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            image = cv2.flip(image,1)\n",
    "            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'RECORDING', (640,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            cv2.putText(image, str((sequence_length-frame_num)//10+1), (870,650), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 5, cv2.LINE_AA)\n",
    "            \n",
    "            image = cv2.resize(image, (1280,960),interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                stop=True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "        bigresults.append(tempresults)\n",
    "        sequence+=1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    for sequence,results in enumerate(bigresults):\n",
    "        for frame,result in enumerate(results):\n",
    "            posepoints=result.pose_landmarks.landmark if result.pose_landmarks else nopose\n",
    "            leftpoints=result.left_hand_landmarks.landmark if result.left_hand_landmarks else noleft\n",
    "            rightpoints=result.right_hand_landmarks.landmark if result.right_hand_landmarks else noright\n",
    "            if result.pose_landmarks:\n",
    "                for i in range(len(posepoints)):\n",
    "                    if(i>18):\n",
    "                        break\n",
    "                    poselist[0][frame][i]=posepoints[i].x\n",
    "                    poselist[1][frame][i]=posepoints[i].y\n",
    "                    poselist[2][frame][i]=posepoints[i].z\n",
    "                poselist[0][frame][17]=posepoints[23].x; poselist[0][frame][18]=posepoints[24].x;\n",
    "                poselist[1][frame][17]=posepoints[23].y; poselist[1][frame][18]=posepoints[24].y;\n",
    "                poselist[2][frame][17]=posepoints[23].z; poselist[2][frame][18]=posepoints[24].z;\n",
    "            else:\n",
    "                poselist[0][frame]=torch.zeros(18)\n",
    "                poselist[1][frame]=torch.zeros(18)\n",
    "                poselist[2][frame]=torch.zeros(18)\n",
    "            if result.left_hand_landmarks:\n",
    "                for i in range(len(leftpoints)):\n",
    "                    leftlist[0][frame][i]=leftpoints[i].x\n",
    "                    leftlist[1][frame][i]=leftpoints[i].y\n",
    "                    leftlist[2][frame][i]=leftpoints[i].z\n",
    "            else:\n",
    "                leftlist[0][frame]=torch.zeros(21)\n",
    "                leftlist[1][frame]=torch.zeros(21)\n",
    "                leftlist[2][frame]=torch.zeros(21)\n",
    "            if result.right_hand_landmarks:\n",
    "                for i in range(len(rightpoints)):\n",
    "                    rightlist[0][frame][i]=rightpoints[i].x\n",
    "                    rightlist[1][frame][i]=rightpoints[i].y\n",
    "                    rightlist[2][frame][i]=rightpoints[i].z\n",
    "            else:\n",
    "                rightlist[0][frame]=torch.zeros(21)\n",
    "                rightlist[1][frame]=torch.zeros(21)\n",
    "                rightlist[2][frame]=torch.zeros(21)\n",
    "                \n",
    "        biglist[sequence]=torch.cat((leftlist,poselist,rightlist),2)\n",
    "\n",
    "# torch.save(biglist,\"test_data/\"+action+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN top-5 Predictions:\n",
      "down\n",
      "among\n",
      "increase\n",
      "spill\n",
      "try\n",
      "\n",
      "NLP Scores for each word:\n",
      "down tensor(7.6291)\n",
      "among tensor(-1.9091)\n",
      "increase tensor(6.5163)\n",
      "spill tensor(13.6828)\n",
      "try tensor(6.7440)\n",
      "\n",
      "final result: spill\n"
     ]
    }
   ],
   "source": [
    "sent = 'Place super glue underneath the rim of the jar lid and seal it down tight so the items won\\'t <mask>'\n",
    "\n",
    "biglist = biglist.cuda()\n",
    "y_hat= getGCNpred(biglist)\n",
    "top5=torch.topk(y_hat,5)\n",
    "print('GCN top-5 Predictions:')\n",
    "for i in top5.indices:\n",
    "    print(a[i][:-3])\n",
    "    \n",
    "print('\\nNLP Scores for each word:')\n",
    "input_ids, mask_idx = encode(electra_tokenizer, sent, add_special_tokens=True)\n",
    "with torch.no_grad():\n",
    "    predict = electra_model(input_ids)[0]   \n",
    "\n",
    "top_word_idx = top5.indices.tolist()\n",
    "top_word_values = []\n",
    "top_words = []\n",
    "for widx in top_word_idx:\n",
    "    w = a[widx][:-3]\n",
    "    top_words.append(w)\n",
    "    top_word_values.append(predict[0, mask_idx, :][electrakeys[w]])\n",
    "    print(w, top_word_values[-1])\n",
    "\n",
    "predwordidx = int(torch.argmax(torch.tensor(top_word_values),dim=0))\n",
    "word2add = top_words[predwordidx]\n",
    "\n",
    "print()\n",
    "print('final result: '+ word2add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([9.5400e-01, 4.5979e-02, 6.6823e-06, 4.4749e-06, 3.5661e-06],\n",
       "       device='cuda:0', grad_fn=<TopkBackward>),\n",
       "indices=tensor([ 86,   2,  98,  85, 101], device='cuda:0'))"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing GCN vs GCN+NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n",
    "electra_model = ElectraForMaskedLM.from_pretrained('google/electra-small-generator').eval()\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for subdir, dirs, files in os.walk('./test_sents'):\n",
    "    b=files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrakeys = dict()\n",
    "for w in a:\n",
    "    if w == 't-shirt.pt':\n",
    "        word = 'shirt.pt'\n",
    "    else:\n",
    "        word = w\n",
    "    for i in range(9999999):\n",
    "        if ''.join(electra_tokenizer.decode(i).split()) == word[:-3].lower():\n",
    "            electrakeys[w[:-3]] = i\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = open(\"sents.txt\", \"r\")\n",
    "sentences = sents.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpcount = 0\n",
    "noobcount =0\n",
    "total = 0\n",
    "for sentence in sentences:\n",
    "    sent,true =  sentence.split('; ')\n",
    "    \n",
    "    testW = torch.load('./test_sents/'+true+'.pt')\n",
    "    y_hat= getGCNpred(testW)\n",
    "    top5=torch.topk(y_hat,5)\n",
    "\n",
    "\n",
    "    input_ids, mask_idx = encode(electra_tokenizer, sent, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = electra_model(input_ids)[0]   \n",
    "\n",
    "    top_word_idx = top5.indices.tolist()\n",
    "    top_word_values = []\n",
    "    top_words = []\n",
    "    for widx in top_word_idx:\n",
    "        w = a[widx][:-3]\n",
    "        top_words.append(w)\n",
    "        top_word_values.append(predict[0, mask_idx, :][electrakeys[w]])\n",
    "\n",
    "    predwordidx = int(torch.argmax(torch.tensor(top_word_values),dim=0))\n",
    "    word2add = top_words[predwordidx]\n",
    "\n",
    "    if word2add == true:\n",
    "        nlpcount+=1\n",
    "\n",
    "    top1w = a[int(top5.indices[0])][:-3]\n",
    "\n",
    "    if top1w == true:\n",
    "        noobcount+=1\n",
    "\n",
    "    total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN + NLP Combination: 97.87\n",
      "Only GCN Predictions: 78.72\n"
     ]
    }
   ],
   "source": [
    "print(\"GCN + NLP Combination: \" + str(round(nlpcount/total *100,2)))\n",
    "print(\"Only GCN Predictions: \"+ str(round(noobcount/total *100,2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancerslr",
   "language": "python",
   "name": "cancerslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
